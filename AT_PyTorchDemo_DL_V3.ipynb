{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AT PyTorch Demo â€” Deep Learning Fundamentals (V3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "SEED = 7\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_rings(n_per_class=500, r_inner=1.0, r_outer=3.0, noise=0.15):\n",
        "    a1 = 2 * math.pi * np.random.rand(n_per_class)\n",
        "    a2 = 2 * math.pi * np.random.rand(n_per_class)\n",
        "    inner = np.c_[r_inner * np.cos(a1), r_inner * np.sin(a1)] + noise * np.random.randn(n_per_class, 2)\n",
        "    outer = np.c_[r_outer * np.cos(a2), r_outer * np.sin(a2)] + noise * np.random.randn(n_per_class, 2)\n",
        "    X = np.vstack([inner, outer]).astype(np.float32)\n",
        "    y = np.r_[np.zeros(n_per_class), np.ones(n_per_class)].astype(np.int64)\n",
        "    idx = np.random.permutation(len(X))\n",
        "    return torch.from_numpy(X[idx]), torch.from_numpy(y[idx])\n",
        "\n",
        "\n",
        "def make_spiral(n_per_class=500, turns=2.5, noise=0.3):\n",
        "    n = n_per_class\n",
        "    t = np.linspace(0.0, turns * math.pi, n)\n",
        "    r = np.linspace(0.2, 1.0, n)\n",
        "    x1 = r * np.cos(t) + noise * np.random.randn(n)\n",
        "    y1 = r * np.sin(t) + noise * np.random.randn(n)\n",
        "    x2 = r * np.cos(t + math.pi) + noise * np.random.randn(n)\n",
        "    y2 = r * np.sin(t + math.pi) + noise * np.random.randn(n)\n",
        "    X = np.vstack([np.c_[x1, y1], np.c_[x2, y2]]).astype(np.float32)\n",
        "    y = np.r_[np.zeros(n), np.ones(n)].astype(np.int64)\n",
        "    idx = np.random.permutation(len(X))\n",
        "    return torch.from_numpy(X[idx]), torch.from_numpy(y[idx])\n",
        "\n",
        "\n",
        "def split_loaders(X, y, batch_size=128, val_ratio=0.2):\n",
        "    n = len(X)\n",
        "    n_val = int(n * val_ratio)\n",
        "    perm = torch.randperm(n)\n",
        "    val_idx, train_idx = perm[:n_val], perm[n_val:]\n",
        "    train_ds = TensorDataset(X[train_idx], y[train_idx])\n",
        "    val_ds = TensorDataset(X[val_idx], y[val_idx])\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
        "        DataLoader(val_ds, batch_size=batch_size, shuffle=False),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, width=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, width), nn.ReLU(),\n",
        "            nn.Linear(width, width), nn.ReLU(),\n",
        "            nn.Linear(width, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def accuracy(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=200, lr=1e-3):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_acc\": []}\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        train_loss = running / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        correct, n = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                correct += (logits.argmax(1) == yb).sum().item()\n",
        "                n += xb.size(0)\n",
        "        val_acc = correct / n\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        if (epoch + 1) % 25 == 0:\n",
        "            print(f\"Epoch {epoch+1:03d} | loss {train_loss:.3f} | val acc {val_acc:.3f}\")\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: rings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xr, yr = make_rings(n_per_class=700, noise=0.12)\n",
        "train_r, val_r = split_loaders(Xr, yr, batch_size=128, val_ratio=0.2)\n",
        "\n",
        "model_r = SimpleMLP(width=64)\n",
        "hist_r = train(model_r, train_r, val_r, epochs=220, lr=1.5e-3)\n",
        "max(hist_r[\"val_acc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: spiral\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xs, ys = make_spiral(n_per_class=800, turns=3.0, noise=0.25)\n",
        "train_s, val_s = split_loaders(Xs, ys, batch_size=128, val_ratio=0.2)\n",
        "\n",
        "model_s = SimpleMLP(width=96)\n",
        "hist_s = train(model_s, train_s, val_s, epochs=260, lr=1e-3)\n",
        "max(hist_s[\"val_acc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- Keep it simple: one can start with a small MLP and adjust width/epochs.\n",
        "- Slightly more epochs for the spiral help it converge.\n",
        "- Adam is a good default; learning rate around 1e-3 is usually fine.\n",
        "- Reported accuracy is from a held-out validation split.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
